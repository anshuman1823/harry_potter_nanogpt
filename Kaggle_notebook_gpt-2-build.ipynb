{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11933033,"sourceType":"datasetVersion","datasetId":7502340}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch numpy transformers datasets tiktoken wandb tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:28:07.122526Z","iopub.execute_input":"2025-05-25T18:28:07.122766Z","iopub.status.idle":"2025-05-25T18:29:19.401311Z","shell.execute_reply.started":"2025-05-25T18:28:07.122741Z","shell.execute_reply":"2025-05-25T18:29:19.400435Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec (from torch)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Prepare Data","metadata":{}},{"cell_type":"code","source":"\"\"\"\nPrepare the Shakespeare dataset for character-level language modeling.\nSo instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\nWill save train.bin, val.bin containing the ids, and meta.pkl containing the\nencoder and decoder and some other related info.\n\"\"\"\nimport os\nimport pickle\nimport requests\nimport numpy as np\n\n# download the tiny dataset\ninput_file_path = \"/kaggle/input/harry-potter-text/harrypotter.txt\"\n\nif not os.path.exists(input_file_path):\n    print(f\"input text file path dne: {input_file_path}\")\n\nwith open(input_file_path, 'r') as f:\n    data = f.read()\nprint(f\"length of dataset in characters: {len(data):,}\")\n\n# get all the unique characters that occur in this text\nchars = sorted(list(set(data)))\nvocab_size = len(chars)\nprint(\"all the unique characters:\", ''.join(chars))\nprint(f\"vocab size: {vocab_size:,}\")\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\ndef encode(s):\n    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndef decode(l):\n    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# create the train and test splits\nn = len(data)\ntrain_data = data[:int(n*0.9)]\nval_data = data[int(n*0.9):]\n\n# encode both to integers\ntrain_ids = encode(train_data)\nval_ids = encode(val_data)\nprint(f\"train has {len(train_ids):,} tokens\")\nprint(f\"val has {len(val_ids):,} tokens\")\n\n# export to bin files\ntrain_ids = np.array(train_ids, dtype=np.uint16)\nval_ids = np.array(val_ids, dtype=np.uint16)\ntrain_ids.tofile(os.path.join(os.getcwd(), 'train.bin'))\nval_ids.tofile(os.path.join(os.getcwd(), 'val.bin'))\n\n# save the meta information as well, to help us encode/decode later\nmeta = {\n    'vocab_size': vocab_size,\n    'itos': itos,\n    'stoi': stoi,\n}\nwith open(os.path.join(os.getcwd(), 'meta.pkl'), 'wb') as f:\n    pickle.dump(meta, f)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:29:47.623936Z","iopub.execute_input":"2025-05-25T18:29:47.624200Z","iopub.status.idle":"2025-05-25T18:29:48.436533Z","shell.execute_reply.started":"2025-05-25T18:29:47.624172Z","shell.execute_reply":"2025-05-25T18:29:48.435929Z"}},"outputs":[{"name":"stdout","text":"length of dataset in characters: 6,283,888\nall the unique characters: \n\f !&(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz©Áßéù–—‘’“”…\nvocab size: 88\ntrain has 5,655,499 tokens\nval has 628,389 tokens\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Model File","metadata":{}},{"cell_type":"code","source":"with open('model.py', \"w\") as file:\n    file.write(\"\"\"\nimport math\nimport inspect\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass LayerNorm(nn.Module):\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu    = nn.GELU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n\n        return logits, loss\n\n    def crop_block_size(self, block_size):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert block_size <= self.config.block_size\n        self.config.block_size = block_size\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n        for block in self.transformer.h:\n            if hasattr(block.attn, 'bias'):\n                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        override_args = override_args or {} # default to empty dict\n        # only dropout can be overridden see more notes below\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        config_args['bias'] = True # always True for GPT model checkpoints\n        # we can override the dropout rate, if desired\n        if 'dropout' in override_args:\n            print(f\"overriding dropout rate to {override_args['dropout']}\")\n            config_args['dropout'] = override_args['dropout']\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        # start with all of the candidate parameters\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        # filter out those that do not require grad\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n        print(f\"using fused AdamW: {use_fused}\")\n\n        return optimizer\n\n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = self.get_num_params()\n        cfg = self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        # express our flops throughput as ratio of A100 bfloat16 peak flops\n        flops_achieved = flops_per_iter * (1.0/dt) # per second\n        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n        mfu = flops_achieved / flops_promised\n        return mfu\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits, _ = self(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx    \n\"\"\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:29:48.771672Z","iopub.execute_input":"2025-05-25T18:29:48.771980Z","iopub.status.idle":"2025-05-25T18:29:48.782683Z","shell.execute_reply.started":"2025-05-25T18:29:48.771961Z","shell.execute_reply":"2025-05-25T18:29:48.782020Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Config File","metadata":{}},{"cell_type":"code","source":"# train a miniature character-level shakespeare model\n# good for debugging and playing on macbooks and such\n\nproject_name = \"harry_potter_char\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:29:49.346676Z","iopub.execute_input":"2025-05-25T18:29:49.347327Z","iopub.status.idle":"2025-05-25T18:29:49.350564Z","shell.execute_reply.started":"2025-05-25T18:29:49.347302Z","shell.execute_reply":"2025-05-25T18:29:49.349890Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"out_dir = 'out-' + project_name\neval_interval = 250 # keep frequent because we'll overfit\neval_iters = 200\nlog_interval = 10 # don't print too too often\n\n# we expect to overfit on this small dataset, so only save when val improves\nalways_save_checkpoint = False\n\nwandb_log = False # override via command line if you like\nwandb_project = project_name\nwandb_run_name = 'mini-gpt'\n\ndataset = project_name\ngradient_accumulation_steps = 1\nbatch_size = 64\nblock_size = 256 # context of up to 256 previous characters\n\n# baby GPT model :)\nn_layer = 6\nn_head = 6\nn_embd = 384\ndropout = 0.2\n\nlearning_rate = 1e-3 # with baby networks can afford to go a bit higher\nmax_iters = 5000\nlr_decay_iters = 5000 # make equal to max_iters usually\nmin_lr = 1e-4 # learning_rate / 10 usually\nbeta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n\nwarmup_iters = 100 # not super necessary potentially\n\n# on macbook also add\n# device = 'cpu'  # run on cpu only\n# compile = False # do not torch compile the model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:29:51.135068Z","iopub.execute_input":"2025-05-25T18:29:51.135524Z","iopub.status.idle":"2025-05-25T18:29:51.140205Z","shell.execute_reply.started":"2025-05-25T18:29:51.135504Z","shell.execute_reply":"2025-05-25T18:29:51.139548Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"with open(os.path.join(os.getcwd(), 'config.py'), \"w\") as file:\n    file.write(\"\"\"project_name = \"harry_potter_char\"\nout_dir = 'out-' + project_name\neval_interval = 250 # keep frequent because we'll overfit\neval_iters = 200\nlog_interval = 10 # don't print too too often\n\n# we expect to overfit on this small dataset, so only save when val improves\nalways_save_checkpoint = False\n\nwandb_log = False # override via command line if you like\nwandb_project = project_name\nwandb_run_name = 'mini-gpt'\n\ndataset = project_name\ngradient_accumulation_steps = 1\nbatch_size = 64\nblock_size = 256 # context of up to 256 previous characters\n\n# baby GPT model :)\nn_layer = 6\nn_head = 6\nn_embd = 384\ndropout = 0.2\n\nlearning_rate = 1e-3 # with baby networks can afford to go a bit higher\nmax_iters = 5000\nlr_decay_iters = 5000 # make equal to max_iters usually\nmin_lr = 1e-4 # learning_rate / 10 usually\nbeta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n\nwarmup_iters = 100 # not super necessary potentially\n\n# on macbook also add\n# device = 'cpu'  # run on cpu only\n# compile = False # do not torch compile the model\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:29:51.397085Z","iopub.execute_input":"2025-05-25T18:29:51.397679Z","iopub.status.idle":"2025-05-25T18:29:51.402018Z","shell.execute_reply.started":"2025-05-25T18:29:51.397658Z","shell.execute_reply":"2025-05-25T18:29:51.401340Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Train code","metadata":{}},{"cell_type":"code","source":"os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:33:00.097184Z","iopub.execute_input":"2025-05-25T18:33:00.097736Z","iopub.status.idle":"2025-05-25T18:33:00.101016Z","shell.execute_reply.started":"2025-05-25T18:33:00.097696Z","shell.execute_reply":"2025-05-25T18:33:00.100454Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"with open(os.path.join(os.getcwd(), 'train.py'), \"w\") as file:\n    file.write(\"\"\"\nimport os\nimport time\nimport math\nimport pickle\nfrom contextlib import nullcontext\n\nimport numpy as np\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\n\nfrom model import GPTConfig, GPT\n\n# -----------------------------------------------------------------------------\n# default config values designed to train a gpt2 (124M) on OpenWebText\n# I/O\nout_dir = 'out'\neval_interval = 2000\nlog_interval = 1\neval_iters = 200\neval_only = False # if True, script exits right after the first eval\nalways_save_checkpoint = True # if True, always save a checkpoint after each eval\ninit_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n# wandb logging\nwandb_log = False # disabled by default\nwandb_project = 'owt'\nwandb_run_name = 'gpt2' # 'run' + str(time.time())\n# data\ndataset = 'openwebtext'\ngradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\nbatch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\nblock_size = 1024\n# model\nn_layer = 12\nn_head = 12\nn_embd = 768\ndropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\nbias = False # do we use bias inside LayerNorm and Linear layers?\n# adamw optimizer\nlearning_rate = 6e-4 # max learning rate\nmax_iters = 600000 # total number of training iterations\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n# learning rate decay settings\ndecay_lr = True # whether to decay the learning rate\nwarmup_iters = 2000 # how many steps to warm up for\nlr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\nmin_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n# DDP settings\nbackend = 'nccl' # 'nccl', 'gloo', etc.\n# system\ndevice = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\ncompile = True # use PyTorch 2.0 to compile the model to be faster\n# -----------------------------------------------------------------------------\nconfig_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\nexec(open('configurator.py').read()) # overrides from command line or config file\nconfig = {k: globals()[k] for k in config_keys} # will be useful for logging\n# -----------------------------------------------------------------------------\n\n# various inits, derived attributes, I/O setup\nddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\nif ddp:\n    init_process_group(backend=backend)\n    ddp_rank = int(os.environ['RANK'])\n    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n    ddp_world_size = int(os.environ['WORLD_SIZE'])\n    device = f'cuda:{ddp_local_rank}'\n    torch.cuda.set_device(device)\n    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n    seed_offset = ddp_rank # each process gets a different seed\n    # world_size number of processes will be training simultaneously, so we can scale\n    # down the desired gradient accumulation iterations per process proportionally\n    assert gradient_accumulation_steps % ddp_world_size == 0\n    gradient_accumulation_steps //= ddp_world_size\nelse:\n    # if not ddp, we are running on a single gpu, and one process\n    master_process = True\n    seed_offset = 0\n    ddp_world_size = 1\ntokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\nprint(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n\nif master_process:\n    os.makedirs(out_dir, exist_ok=True)\ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\ntorch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n# note: float16 data type will automatically use a GradScaler\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\n# poor man's data loader\ndata_dir = os.path.join('data', dataset)\ndef get_batch(split):\n    # We recreate np.memmap every batch to avoid a memory leak, as per\n    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n    if split == 'train':\n        data = np.memmap(os.path.join(os.getcwd(), 'train.bin'), dtype=np.uint16, mode='r')\n    else:\n        data = np.memmap(os.path.join(os.getcwd(), 'val.bin'), dtype=np.uint16, mode='r')\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if device_type == 'cuda':\n        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n    else:\n        x, y = x.to(device), y.to(device)\n    return x, y\n\n# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\niter_num = 0\nbest_val_loss = 1e9\n\n# attempt to derive vocab_size from the dataset\nmeta_path = os.path.join(os.getcwd(), 'meta.pkl')\nmeta_vocab_size = None\nif os.path.exists(meta_path):\n    with open(meta_path, 'rb') as f:\n        meta = pickle.load(f)\n    meta_vocab_size = meta['vocab_size']\n    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n\n# model init\nmodel_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\nif init_from == 'scratch':\n    # init a new model from scratch\n    print(\"Initializing a new model from scratch\")\n    # determine the vocab size we'll use for from-scratch training\n    if meta_vocab_size is None:\n        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n    gptconf = GPTConfig(**model_args)\n    model = GPT(gptconf)\nelif init_from == 'resume':\n    print(f\"Resuming training from {out_dir}\")\n    # resume training from a checkpoint.\n    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n    checkpoint = torch.load(ckpt_path, map_location=device)\n    checkpoint_model_args = checkpoint['model_args']\n    # force these config attributes to be equal otherwise we can't even resume training\n    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n        model_args[k] = checkpoint_model_args[k]\n    # create the model\n    gptconf = GPTConfig(**model_args)\n    model = GPT(gptconf)\n    state_dict = checkpoint['model']\n    # fix the keys of the state dictionary :(\n    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n    unwanted_prefix = '_orig_mod.'\n    for k,v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n    model.load_state_dict(state_dict)\n    iter_num = checkpoint['iter_num']\n    best_val_loss = checkpoint['best_val_loss']\nelif init_from.startswith('gpt2'):\n    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n    # initialize from OpenAI GPT-2 weights\n    override_args = dict(dropout=dropout)\n    model = GPT.from_pretrained(init_from, override_args)\n    # read off the created config params, so we can store them into checkpoint correctly\n    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n        model_args[k] = getattr(model.config, k)\n# crop down the model block size if desired, using model surgery\nif block_size < model.config.block_size:\n    model.crop_block_size(block_size)\n    model_args['block_size'] = block_size # so that the checkpoint will have the right value\nmodel.to(device)\n\n# initialize a GradScaler. If enabled=False scaler is a no-op\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n\n# optimizer\noptimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\nif init_from == 'resume':\n    optimizer.load_state_dict(checkpoint['optimizer'])\ncheckpoint = None # free up memory\n\n# compile the model\nif compile:\n    print(\"compiling the model... (takes a ~minute)\")\n    unoptimized_model = model\n    model = torch.compile(model, backend=\"eager\") # requires PyTorch 2.0\n\n# wrap model into DDP container\nif ddp:\n    model = DDP(model, device_ids=[ddp_local_rank])\n\n# helps estimate an arbitrarily accurate loss over either split using many batches\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            with ctx:\n                logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# learning rate decay scheduler (cosine with warmup)\ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_iters:\n        return learning_rate * (it + 1) / (warmup_iters + 1)\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > lr_decay_iters:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n    return min_lr + coeff * (learning_rate - min_lr)\n\n# logging\nif wandb_log and master_process:\n    import wandb\n    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n\n# training loop\nX, Y = get_batch('train') # fetch the very first batch\nt0 = time.time()\nlocal_iter_num = 0 # number of iterations in the lifetime of this process\nraw_model = model.module if ddp else model # unwrap DDP container if needed\nrunning_mfu = -1.0\nwhile True:\n\n    # determine and set the learning rate for this iteration\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n    # evaluate the loss on train/val sets and write checkpoints\n    if iter_num % eval_interval == 0 and master_process:\n        losses = estimate_loss()\n        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        if wandb_log:\n            wandb.log({\n                \"iter\": iter_num,\n                \"train/loss\": losses['train'],\n                \"val/loss\": losses['val'],\n                \"lr\": lr,\n                \"mfu\": running_mfu*100, # convert to percentage\n            })\n        if losses['val'] < best_val_loss or always_save_checkpoint:\n            best_val_loss = losses['val']\n            if iter_num > 0:\n                checkpoint = {\n                    'model': raw_model.state_dict(),\n                    'optimizer': optimizer.state_dict(),\n                    'model_args': model_args,\n                    'iter_num': iter_num,\n                    'best_val_loss': best_val_loss,\n                    'config': config,\n                }\n                print(f\"saving checkpoint to {out_dir}\")\n                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n    if iter_num == 0 and eval_only:\n        break\n\n    # forward backward update, with optional gradient accumulation to simulate larger batch size\n    # and using the GradScaler if data type is float16\n    for micro_step in range(gradient_accumulation_steps):\n        if ddp:\n            # in DDP training we only need to sync gradients at the last micro step.\n            # the official way to do this is with model.no_sync() context manager, but\n            # I really dislike that this bloats the code and forces us to repeat code\n            # looking at the source of that context manager, it just toggles this variable\n            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n        with ctx:\n            logits, loss = model(X, Y)\n            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n        X, Y = get_batch('train')\n        # backward pass, with gradient scaling if training in fp16\n        scaler.scale(loss).backward()\n    # clip the gradient\n    if grad_clip != 0.0:\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n    # step the optimizer and scaler if training in fp16\n    scaler.step(optimizer)\n    scaler.update()\n    # flush the gradients as soon as we can, no need for this memory anymore\n    optimizer.zero_grad(set_to_none=True)\n\n    # timing and logging\n    t1 = time.time()\n    dt = t1 - t0\n    t0 = t1\n    if iter_num % log_interval == 0 and master_process:\n        # get loss as float. note: this is a CPU-GPU sync point\n        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n        lossf = loss.item() * gradient_accumulation_steps\n        if local_iter_num >= 5: # let the training loop settle a bit\n            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n    iter_num += 1\n    local_iter_num += 1\n\n    # termination conditions\n    if iter_num > max_iters:\n        break\n\nif ddp:\n    destroy_process_group()\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:33:04.072998Z","iopub.execute_input":"2025-05-25T18:33:04.073444Z","iopub.status.idle":"2025-05-25T18:33:04.083139Z","shell.execute_reply.started":"2025-05-25T18:33:04.073423Z","shell.execute_reply":"2025-05-25T18:33:04.082468Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Configurator code","metadata":{}},{"cell_type":"code","source":"with open(\"configurator.py\", \"w\") as file:\n    file.write(\"\"\"\nimport sys\nfrom ast import literal_eval\n\nfor arg in sys.argv[1:]:\n    if '=' not in arg:\n        # assume it's the name of a config file\n        assert not arg.startswith('--')\n        config_file = arg\n        print(f\"Overriding config with {config_file}:\")\n        with open(config_file) as f:\n            print(f.read())\n        exec(open(config_file).read())\n    else:\n        # assume it's a --key=value argument\n        assert arg.startswith('--')\n        key, val = arg.split('=')\n        key = key[2:]\n        if key in globals():\n            try:\n                # attempt to eval it it (e.g. if bool, number, or etc)\n                attempt = literal_eval(val)\n            except (SyntaxError, ValueError):\n                # if that goes wrong, just use the string\n                attempt = val\n            # ensure the types match ok\n            assert type(attempt) == type(globals()[key])\n            # cross fingers\n            print(f\"Overriding: {key} = {attempt}\")\n            globals()[key] = attempt\n        else:\n            raise ValueError(f\"Unknown config key: {key}\")    \n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:33:04.780783Z","iopub.execute_input":"2025-05-25T18:33:04.781001Z","iopub.status.idle":"2025-05-25T18:33:04.785351Z","shell.execute_reply.started":"2025-05-25T18:33:04.780987Z","shell.execute_reply":"2025-05-25T18:33:04.784745Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!python train.py config.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:10:59.551764Z","iopub.execute_input":"2025-05-25T19:10:59.552279Z"}},"outputs":[{"name":"stdout","text":"Overriding config with config.py:\nproject_name = \"harry_potter_char\"\nout_dir = 'out-' + project_name\neval_interval = 250 # keep frequent because we'll overfit\neval_iters = 200\nlog_interval = 10 # don't print too too often\n\n# we expect to overfit on this small dataset, so only save when val improves\nalways_save_checkpoint = False\n\nwandb_log = False # override via command line if you like\nwandb_project = project_name\nwandb_run_name = 'mini-gpt'\n\ndataset = project_name\ngradient_accumulation_steps = 1\nbatch_size = 64\nblock_size = 256 # context of up to 256 previous characters\n\n# baby GPT model :)\nn_layer = 6\nn_head = 6\nn_embd = 384\ndropout = 0.2\n\nlearning_rate = 1e-3 # with baby networks can afford to go a bit higher\nmax_iters = 5000\nlr_decay_iters = 5000 # make equal to max_iters usually\nmin_lr = 1e-4 # learning_rate / 10 usually\nbeta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n\nwarmup_iters = 100 # not super necessary potentially\n\n# on macbook also add\n# device = 'cpu'  # run on cpu only\n# compile = False # do not torch compile the model\ntokens per iteration will be: 16,384\nfound vocab_size = 88 (inside /kaggle/working/meta.pkl)\nInitializing a new model from scratch\nnumber of parameters: 10.66M\n/kaggle/working/train.py:179: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\nnum decayed parameter tensors: 26, with 10,748,928 parameters\nnum non-decayed parameter tensors: 13, with 4,992 parameters\nusing fused AdamW: True\ncompiling the model... (takes a ~minute)\nstep 0: train loss 4.5586, val loss 4.5605\niter 0: loss 4.5542, time 73852.23ms, mfu -100.00%\niter 10: loss 3.1930, time 547.41ms, mfu 0.68%\niter 20: loss 2.8104, time 556.00ms, mfu 0.68%\niter 30: loss 2.6419, time 561.60ms, mfu 0.68%\niter 40: loss 2.5833, time 562.66ms, mfu 0.68%\niter 50: loss 2.5479, time 567.23ms, mfu 0.68%\niter 60: loss 2.5107, time 571.11ms, mfu 0.67%\niter 70: loss 2.4858, time 578.51ms, mfu 0.67%\niter 80: loss 2.4951, time 575.62ms, mfu 0.67%\niter 90: loss 2.4969, time 582.13ms, mfu 0.67%\niter 100: loss 2.5273, time 576.55ms, mfu 0.66%\niter 110: loss 2.4712, time 574.21ms, mfu 0.66%\niter 120: loss 2.4486, time 571.03ms, mfu 0.66%\niter 130: loss 2.4521, time 566.99ms, mfu 0.66%\niter 140: loss 2.4154, time 564.15ms, mfu 0.66%\niter 150: loss 2.3745, time 563.79ms, mfu 0.66%\niter 160: loss 2.3618, time 562.75ms, mfu 0.66%\niter 170: loss 2.3398, time 563.67ms, mfu 0.66%\niter 180: loss 2.2430, time 563.21ms, mfu 0.66%\niter 190: loss 2.1973, time 563.49ms, mfu 0.66%\niter 200: loss 2.1518, time 565.14ms, mfu 0.66%\niter 210: loss 2.0904, time 565.52ms, mfu 0.66%\niter 220: loss 2.0612, time 570.43ms, mfu 0.66%\niter 230: loss 2.0361, time 570.58ms, mfu 0.66%\niter 240: loss 2.0192, time 569.60ms, mfu 0.66%\nstep 250: train loss 1.8987, val loss 1.9068\nsaving checkpoint to out-harry_potter_char\niter 250: loss 1.9702, time 80416.32ms, mfu 0.59%\niter 260: loss 1.9366, time 565.84ms, mfu 0.60%\niter 270: loss 1.8915, time 566.71ms, mfu 0.61%\niter 280: loss 1.8780, time 566.91ms, mfu 0.61%\niter 290: loss 1.9060, time 567.45ms, mfu 0.62%\niter 300: loss 1.8640, time 569.16ms, mfu 0.62%\niter 310: loss 1.8567, time 566.44ms, mfu 0.62%\niter 320: loss 1.8365, time 569.59ms, mfu 0.63%\niter 330: loss 1.7819, time 567.07ms, mfu 0.63%\niter 340: loss 1.7793, time 569.22ms, mfu 0.63%\niter 350: loss 1.7368, time 567.44ms, mfu 0.63%\niter 360: loss 1.7530, time 569.51ms, mfu 0.64%\niter 370: loss 1.7340, time 567.36ms, mfu 0.64%\niter 380: loss 1.7376, time 573.45ms, mfu 0.64%\niter 390: loss 1.6920, time 570.89ms, mfu 0.64%\niter 400: loss 1.6998, time 566.66ms, mfu 0.64%\niter 410: loss 1.6829, time 571.67ms, mfu 0.64%\niter 420: loss 1.6759, time 569.11ms, mfu 0.65%\niter 430: loss 1.6621, time 567.60ms, mfu 0.65%\niter 440: loss 1.6487, time 571.52ms, mfu 0.65%\niter 450: loss 1.6456, time 572.26ms, mfu 0.65%\niter 460: loss 1.6346, time 569.26ms, mfu 0.65%\niter 470: loss 1.6372, time 570.44ms, mfu 0.65%\niter 480: loss 1.6192, time 567.62ms, mfu 0.65%\niter 490: loss 1.5842, time 567.64ms, mfu 0.65%\nstep 500: train loss 1.4923, val loss 1.5216\nsaving checkpoint to out-harry_potter_char\niter 500: loss 1.6060, time 80508.89ms, mfu 0.59%\niter 510: loss 1.5839, time 567.66ms, mfu 0.59%\niter 520: loss 1.5674, time 570.06ms, mfu 0.60%\niter 530: loss 1.5335, time 567.15ms, mfu 0.60%\niter 540: loss 1.5709, time 569.45ms, mfu 0.61%\niter 550: loss 1.5368, time 570.15ms, mfu 0.61%\niter 560: loss 1.5115, time 569.26ms, mfu 0.62%\niter 570: loss 1.5465, time 566.76ms, mfu 0.62%\niter 580: loss 1.5542, time 565.78ms, mfu 0.63%\niter 590: loss 1.5029, time 567.36ms, mfu 0.63%\niter 600: loss 1.4689, time 566.80ms, mfu 0.63%\niter 610: loss 1.4634, time 568.27ms, mfu 0.63%\niter 620: loss 1.4606, time 566.66ms, mfu 0.64%\niter 630: loss 1.5141, time 571.95ms, mfu 0.64%\niter 640: loss 1.5089, time 568.68ms, mfu 0.64%\niter 650: loss 1.4621, time 567.90ms, mfu 0.64%\niter 660: loss 1.4760, time 566.84ms, mfu 0.64%\niter 670: loss 1.4379, time 570.62ms, mfu 0.64%\niter 680: loss 1.4227, time 571.03ms, mfu 0.65%\niter 690: loss 1.4813, time 566.35ms, mfu 0.65%\niter 700: loss 1.4489, time 566.90ms, mfu 0.65%\niter 710: loss 1.4049, time 569.01ms, mfu 0.65%\niter 720: loss 1.4297, time 568.84ms, mfu 0.65%\niter 730: loss 1.4476, time 568.19ms, mfu 0.65%\niter 740: loss 1.4260, time 567.58ms, mfu 0.65%\nstep 750: train loss 1.3181, val loss 1.3668\nsaving checkpoint to out-harry_potter_char\niter 750: loss 1.4209, time 80501.46ms, mfu 0.59%\niter 760: loss 1.4233, time 566.93ms, mfu 0.59%\niter 770: loss 1.3754, time 569.44ms, mfu 0.60%\niter 780: loss 1.3480, time 568.95ms, mfu 0.60%\niter 790: loss 1.3608, time 565.59ms, mfu 0.61%\niter 800: loss 1.3839, time 569.02ms, mfu 0.61%\niter 810: loss 1.3493, time 566.27ms, mfu 0.62%\niter 820: loss 1.3597, time 569.57ms, mfu 0.62%\niter 830: loss 1.3975, time 566.37ms, mfu 0.63%\niter 840: loss 1.3834, time 569.55ms, mfu 0.63%\niter 850: loss 1.3388, time 566.12ms, mfu 0.63%\niter 860: loss 1.3640, time 569.88ms, mfu 0.63%\niter 870: loss 1.3650, time 567.26ms, mfu 0.64%\niter 880: loss 1.3410, time 569.96ms, mfu 0.64%\niter 890: loss 1.3376, time 568.51ms, mfu 0.64%\niter 900: loss 1.3500, time 566.17ms, mfu 0.64%\niter 910: loss 1.3805, time 571.13ms, mfu 0.64%\niter 920: loss 1.2906, time 565.13ms, mfu 0.64%\niter 930: loss 1.3368, time 568.75ms, mfu 0.65%\niter 940: loss 1.3199, time 568.53ms, mfu 0.65%\niter 950: loss 1.3570, time 569.67ms, mfu 0.65%\niter 960: loss 1.3239, time 569.07ms, mfu 0.65%\niter 970: loss 1.3098, time 569.47ms, mfu 0.65%\niter 980: loss 1.2942, time 566.06ms, mfu 0.65%\niter 990: loss 1.2978, time 569.95ms, mfu 0.65%\nstep 1000: train loss 1.2311, val loss 1.2865\nsaving checkpoint to out-harry_potter_char\niter 1000: loss 1.3218, time 80507.96ms, mfu 0.59%\niter 1010: loss 1.3152, time 567.70ms, mfu 0.59%\niter 1020: loss 1.3481, time 567.68ms, mfu 0.60%\niter 1030: loss 1.2627, time 568.72ms, mfu 0.60%\niter 1040: loss 1.2676, time 570.86ms, mfu 0.61%\niter 1050: loss 1.3142, time 571.34ms, mfu 0.61%\niter 1060: loss 1.2696, time 569.68ms, mfu 0.62%\niter 1070: loss 1.2742, time 571.03ms, mfu 0.62%\niter 1080: loss 1.2923, time 563.05ms, mfu 0.63%\niter 1090: loss 1.2637, time 570.67ms, mfu 0.63%\niter 1100: loss 1.3031, time 567.09ms, mfu 0.63%\niter 1110: loss 1.2777, time 568.34ms, mfu 0.63%\niter 1120: loss 1.2801, time 566.76ms, mfu 0.64%\niter 1130: loss 1.2543, time 566.80ms, mfu 0.64%\niter 1140: loss 1.2821, time 569.45ms, mfu 0.64%\niter 1150: loss 1.2726, time 570.57ms, mfu 0.64%\niter 1160: loss 1.2862, time 569.18ms, mfu 0.64%\niter 1170: loss 1.2597, time 568.30ms, mfu 0.64%\niter 1180: loss 1.2665, time 567.39ms, mfu 0.65%\niter 1190: loss 1.2838, time 571.34ms, mfu 0.65%\niter 1200: loss 1.2628, time 572.22ms, mfu 0.65%\niter 1210: loss 1.2437, time 567.03ms, mfu 0.65%\niter 1220: loss 1.2584, time 571.17ms, mfu 0.65%\niter 1230: loss 1.2284, time 567.87ms, mfu 0.65%\niter 1240: loss 1.2770, time 572.12ms, mfu 0.65%\nstep 1250: train loss 1.1760, val loss 1.2385\nsaving checkpoint to out-harry_potter_char\niter 1250: loss 1.2672, time 80631.13ms, mfu 0.58%\niter 1260: loss 1.2364, time 568.33ms, mfu 0.59%\niter 1270: loss 1.2749, time 567.76ms, mfu 0.60%\niter 1280: loss 1.2358, time 566.88ms, mfu 0.60%\niter 1290: loss 1.2603, time 566.15ms, mfu 0.61%\niter 1300: loss 1.2542, time 567.53ms, mfu 0.61%\niter 1310: loss 1.2292, time 567.71ms, mfu 0.62%\niter 1320: loss 1.2264, time 572.55ms, mfu 0.62%\niter 1330: loss 1.2017, time 570.20ms, mfu 0.63%\niter 1340: loss 1.2302, time 570.81ms, mfu 0.63%\niter 1350: loss 1.2376, time 570.10ms, mfu 0.63%\niter 1360: loss 1.2239, time 567.60ms, mfu 0.63%\niter 1370: loss 1.2322, time 567.73ms, mfu 0.64%\niter 1380: loss 1.2145, time 571.81ms, mfu 0.64%\niter 1390: loss 1.2283, time 569.80ms, mfu 0.64%\niter 1400: loss 1.1933, time 567.62ms, mfu 0.64%\niter 1410: loss 1.2198, time 567.56ms, mfu 0.64%\niter 1420: loss 1.2018, time 570.56ms, mfu 0.64%\niter 1430: loss 1.2338, time 566.48ms, mfu 0.65%\niter 1440: loss 1.2090, time 571.62ms, mfu 0.65%\niter 1450: loss 1.2022, time 566.33ms, mfu 0.65%\niter 1460: loss 1.2099, time 570.32ms, mfu 0.65%\niter 1470: loss 1.1956, time 570.07ms, mfu 0.65%\niter 1480: loss 1.1855, time 568.39ms, mfu 0.65%\niter 1490: loss 1.2144, time 567.43ms, mfu 0.65%\nstep 1500: train loss 1.1399, val loss 1.2072\nsaving checkpoint to out-harry_potter_char\niter 1500: loss 1.2385, time 80646.57ms, mfu 0.59%\niter 1510: loss 1.1969, time 569.08ms, mfu 0.59%\niter 1520: loss 1.2207, time 570.46ms, mfu 0.60%\niter 1530: loss 1.2298, time 571.43ms, mfu 0.60%\niter 1540: loss 1.2055, time 570.88ms, mfu 0.61%\niter 1550: loss 1.2012, time 570.72ms, mfu 0.61%\niter 1560: loss 1.1863, time 569.23ms, mfu 0.62%\niter 1570: loss 1.2031, time 570.66ms, mfu 0.62%\niter 1580: loss 1.1780, time 569.11ms, mfu 0.62%\niter 1590: loss 1.2059, time 570.54ms, mfu 0.63%\niter 1600: loss 1.1754, time 569.85ms, mfu 0.63%\niter 1610: loss 1.2333, time 567.13ms, mfu 0.63%\niter 1620: loss 1.1923, time 566.81ms, mfu 0.64%\niter 1630: loss 1.1806, time 569.84ms, mfu 0.64%\niter 1640: loss 1.1876, time 570.69ms, mfu 0.64%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Sample Code","metadata":{}},{"cell_type":"code","source":"with open(\"sample.py\", \"w\") as file:\n    file.write(\"\"\"\nimport os\nimport pickle\nfrom contextlib import nullcontext\nimport torch\nimport tiktoken\nfrom model import GPTConfig, GPT\n\n# -----------------------------------------------------------------------------\ninit_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\nout_dir = 'out' # ignored if init_from is not 'resume'\nstart = \"FILE:prompt.txt\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\nnum_samples = 10 # number of samples to draw\nmax_new_tokens = 500 # number of tokens generated in each sample\ntemperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\ntop_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\nseed = 1337\ndevice = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\ncompile = False # use PyTorch 2.0 to compile the model to be faster\nexec(open('configurator.py').read()) # overrides from command line or config file\n# -----------------------------------------------------------------------------\n\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\ntorch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\n# model\nif init_from == 'resume':\n    # init from a model saved in a specific directory\n    ckpt_path = os.path.join(os.getcwd(), out_dir, 'ckpt.pt')\n    checkpoint = torch.load(ckpt_path, map_location=device)\n    gptconf = GPTConfig(**checkpoint['model_args'])\n    model = GPT(gptconf)\n    state_dict = checkpoint['model']\n    unwanted_prefix = '_orig_mod.'\n    for k,v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n    model.load_state_dict(state_dict)\nelif init_from.startswith('gpt2'):\n    # init from a given GPT-2 model\n    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n\nmodel.eval()\nmodel.to(device)\nif compile:\n    model = torch.compile(model, backend = \"eager\") # requires PyTorch 2.0 (optional)\n\n# look for the meta pickle in case it is available in the dataset folder\nload_meta = False\nif init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n    meta_path = os.path.join(os.getcwd(), 'meta.pkl')\n    load_meta = os.path.exists(meta_path)\nif load_meta:\n    print(f\"Loading meta from {meta_path}...\")\n    with open(meta_path, 'rb') as f:\n        meta = pickle.load(f)\n    # TODO want to make this more general to arbitrary encoder/decoder schemes\n    stoi, itos = meta['stoi'], meta['itos']\n    encode = lambda s: [stoi[c] for c in s]\n    decode = lambda l: ''.join([itos[i] for i in l])\nelse:\n    # ok let's assume gpt-2 encodings by default\n    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n    enc = tiktoken.get_encoding(\"gpt2\")\n    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n    decode = lambda l: enc.decode(l)\n\n# encode the beginning of the prompt\nif start.startswith('FILE:'):\n    with open(start[5:], 'r', encoding='utf-8') as f:\n        start = f.read()\nstart_ids = encode(start)\nx = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n\n# run generation\nwith torch.no_grad():\n    with ctx:\n        for k in range(num_samples):\n            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n            print(decode(y[0].tolist()))\n            print('---------------')\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:56:50.966432Z","iopub.execute_input":"2025-05-25T18:56:50.967104Z","iopub.status.idle":"2025-05-25T18:56:50.973047Z","shell.execute_reply.started":"2025-05-25T18:56:50.967076Z","shell.execute_reply":"2025-05-25T18:56:50.972318Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Prompt","metadata":{}},{"cell_type":"code","source":"prompt = \"Harry kissed Ginny\"\n\nwith open(\"prompt.txt\", \"w\") as file:\n    file.write(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:01:02.765285Z","iopub.execute_input":"2025-05-25T19:01:02.765552Z","iopub.status.idle":"2025-05-25T19:01:02.771210Z","shell.execute_reply.started":"2025-05-25T19:01:02.765533Z","shell.execute_reply":"2025-05-25T19:01:02.769901Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"!python sample.py --out_dir=out-harry_potter_char","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:01:04.072804Z","iopub.execute_input":"2025-05-25T19:01:04.073066Z","iopub.status.idle":"2025-05-25T19:01:33.996860Z","shell.execute_reply.started":"2025-05-25T19:01:04.073048Z","shell.execute_reply":"2025-05-25T19:01:33.996177Z"}},"outputs":[{"name":"stdout","text":"Overriding: out_dir = out-harry_potter_char\nnumber of parameters: 10.66M\nLoading meta from /kaggle/working/meta.pkl...\nHarry kissed Ginny again, who\nwas staring to him\nfrom her to stop off the Cloak and hurried toward the circle in the wall.\nHarry tucked on the class, and Voldemort kept to a man order than because his\nfew minutes from his eyes.\nHarry saw her face, still wearing at the featling table and stretched as he saw the spin of\ndragon of green on the room.\nMolly looked up at the good to see most guess than he should be some at this\nperson.\nThen distructed to his face, the eye was almost unplucked the considered to be a gre\n---------------\nHarry kissed Ginny to the Firebolt.\n“But I thought he did not know the silver of the Daily Prophet, that’s sure you get\nto do it.”\n“So,” said Ron, unconfusionally one of the drawing people\ninto the battom and put the common room. “When you’re here, Harry, and they’re mad\nthe Seeker, we really . . let it to know when you’re Dumbledore when she says the Department Potter chance of now dismouth. You\nknow can kill . . .”\n“What’ve left it,” said Ron, frowning around the collection. “Sir, if you will hear him,\nyou’re g\n---------------\nHarry kissed Ginny at all. He looked desperately to relate the\ndoor steps along the door.\n“We’re going to be thinking to but up,” said Harry.\n“We think he quite for a thin teacher,” said Sirius. “So we didn’t know\nwhere you see you go?”\n“He’s just been starting over the sight. He snooked drifting at the desk of the Daily\nProphet, her face just fell with the pace of them, they heard it fairful. A very good voice to even\nhis eyes opened voice; he could not look sobbing uprecely see for his mouths up a glass\nand the\n---------------\nHarry kissed Ginny.\n“Yeah,” said Professor Sprout. “They’re very probably starting in this all. . . .\nI’m going to keep anymore . .”\n“They’re long ter say when you’d believe they’re business,” said Snape quietly.\n“Of course,” said Ron.\n“But can we can’t want to level me . . .” said Ron, pleased through the room.\n“What d’you make you up at the Ministry of Mysterious will be more than your place to chance with your trunk of him\n— ?”\n“Ron!” said the coffense Quidditch, “you were” said Harry.\n“I wasn’t looking up at \n---------------\nHarry kissed Ginny, who was falling\naround, the other side of subjects to the chair, pair it off it on his glasses behind the\ncentaur sides of them and toward him.\n\n\f\n\fCHAPTER THE SEAY! We should be coming to be under the newd of Magic and Master\nthere was at what he actually know you listening this,” said Mrs. Weasley\nannouncertainly. “Jeeling for have you nearly only to stand action his\nparents with her everyone standings up, and we were all he’d heard what was they not going to\ntalk to see him, and I was holdi\n---------------\nHarry kissed Ginny at him.\n“Walk possible, what Diggory?” she said with a slamy peerful voice\nto preciously.\n“You think it’s all the spell,” said Ron.\n“We’re going to tell them,” said Hermione.\nWhich was looking at a halt past. Harry had very found the statue, who saw Harry\nhad been sure the process green with the bleed into more table, are you?” asked\nRon.\n“Tell me over what?” said Dumbledore. “I snout the Destrait of Magic. Discussing it was\nthere?”\n“You’d need to go behind them, go to pleas when you were to ta\n---------------\nHarry kissed Ginny, who\nwas following him. “Well, if you is not going to summon him to\nhave made them, they seemed to to get you with to go. They were come on, as they\nweren’t heard as Muggle as though they blacked aheart of prophecy. When\nthe text confessed to help what he had were come back to his\nfeet, and she hot and then facing Professor McGonagall, and Harry couldn’t tell off\nthem back to them, and the desk of the trees? There were on the trathere the meantree was performmed to realize and pulled out\nof my \n---------------\nHarry kissed Ginny, who was staring.\nHarry ran heavily showing Winky Thousandtakes with his master’s\ncourtred and bean dark to the present witch other. Yet there were now bald\nthought the end of the air of the Patrol teams lesson: Year of the Quibbler jabbled and\nstopped back to the uncle of the steps had seen it down the previous lump. One of the spell was not spinning\nnow, but it was now. He backed toward himself from when Hermione looked at him,\nwas frustering her, and the tournament, but there was some times \n---------------\nHarry kissed Ginny on it, which was set of corridor, and they reached the real tables had heard\nhim.\n“But then, he has never want to him, as he would have been gone up with\nyou save now what Hermione has put it been about when they can call have been starting\nreally when you’re little with over the many broomsticks’.”\n“Dad you say so you,” said Hagrid, but what I wouldn’t want to be trying, you’d bet too,\nthey’re decided in here? He had not acted the common room for you realized it\nstood . . .”\n“What’s going to t\n---------------\nHarry kissed Ginny’s stomach, which were\npointing at the empty of the man walls.\nThen he was still an old habit —\nHarry had just left his face to what it was looking bit anyway from him,\nthe Hufflepuffs kneckled off toward the lames little now.\nThe good were all stood at him, so though it was act, also the voice was standing at the way\non the comman room.\n“What if you know we must bland up to win the moment —”\n“You’re going on the summon, sure you can tell me it,” said Aunt Petunia. “The Ministry of\nMagic.”\n“I’ll\n---------------\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}